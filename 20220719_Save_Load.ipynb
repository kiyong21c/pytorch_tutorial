{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiyong21c/pytorch_tutorial/blob/main/20220719_Save_Load.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save, load\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "''' 3 DIFFERENT METHODS TO REMEMBER:\n",
        " - torch.save(arg, PATH) # can be model, tensor, or dictionary\n",
        " - torch.load(PATH)\n",
        " - torch.load_state_dict(arg)\n",
        "'''\n",
        "\n",
        "''' 2 DIFFERENT WAYS OF SAVING\n",
        "# 1) lazy way: save whole model\n",
        "torch.save(model, PATH) → 모델전체를 저장\n",
        "# model class must be defined somewhere\n",
        "model = torch.load(PATH) → 저장한경로를 가져와서 모델변수에 할당\n",
        "model.eval() → 평가모드(이미 훈련이 된 후 저장된 모델이므로)\n",
        "\n",
        "# 2) recommended way: save only the state_dict\n",
        "torch.save(model.state_dict(), PATH) → dict형태로 저장된 parameter(tensor타입)만 저장\n",
        "# model must be created again with parameters\n",
        "model = Model(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "'''\n",
        "\n",
        "# 데이터 준비\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) # torch.size([4, 1])\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32) \n",
        "\n",
        "n_samples, n_features = X.shape # torch.Size([4, 1])\n",
        "print(f'samples: {n_samples}, features: {n_features}') # 4, 1\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32) # X_test.shape : torch.Size([1]) 넘파이랑 다른 형식\n",
        "\n",
        "# 모델 및 옵티마이저 정의\n",
        "model = nn.Linear(1, 1)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "# 모델 학습\n",
        "for i in range(200):\n",
        "    # forward\n",
        "    Y_predict = model(X)\n",
        "    loss = ((Y-Y_predict) **2).mean() # scalar로 만들어야함\n",
        "\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if i//20==0:\n",
        "        print(loss.item())\n",
        "\n",
        "# 파라미터 확인\n",
        "parameters = list(model.parameters())\n",
        "weight = parameters[0].item()\n",
        "bias = parameters[1].item()\n",
        "print(f'weights : {weight:.3f}, bias : {bias:.3f}')\n",
        "    \n",
        "\n",
        "# ####################save all ######################################\n",
        "# # save and load entire model\n",
        "\n",
        "# FILE = \"model.pth\"\n",
        "# torch.save(model, FILE) # save(모델인스턴스, 경로)\n",
        "\n",
        "# loaded_model = torch.load(FILE)\n",
        "# loaded_model.eval()\n",
        "\n",
        "# for param in loaded_model.parameters():\n",
        "#     print(param)\n",
        "\n",
        "\n",
        "############save only state dict #########################\n",
        "\n",
        "# save only state dict\n",
        "FILE = \"model.pth\"\n",
        "torch.save(model.state_dict(), FILE)\n",
        "\n",
        "print(model.state_dict())\n",
        "loaded_model = Model(n_input_features=6)\n",
        "loaded_model.load_state_dict(torch.load(FILE)) # it takes the loaded dictionary, not the path file itself\n",
        "loaded_model.eval()\n",
        "\n",
        "print(loaded_model.state_dict())\n",
        "\n",
        "\n",
        "# ###########load checkpoint#####################\n",
        "# learning_rate = 0.01\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# checkpoint = {\n",
        "# \"epoch\": 90,\n",
        "# \"model_state\": model.state_dict(),\n",
        "# \"optim_state\": optimizer.state_dict()\n",
        "# }\n",
        "# print(optimizer.state_dict())\n",
        "# FILE = \"checkpoint.pth\"\n",
        "# torch.save(checkpoint, FILE)\n",
        "\n",
        "# model = Model(n_input_features=6)\n",
        "# optimizer = optimizer = torch.optim.SGD(model.parameters(), lr=0)\n",
        "\n",
        "# checkpoint = torch.load(FILE)\n",
        "# model.load_state_dict(checkpoint['model_state'])\n",
        "# optimizer.load_state_dict(checkpoint['optim_state'])\n",
        "# epoch = checkpoint['epoch']\n",
        "\n",
        "# model.eval()\n",
        "# # - or -\n",
        "# # model.train()\n",
        "\n",
        "# print(optimizer.state_dict())\n",
        "\n",
        "# # Remember that you must call model.eval() to set dropout and batch normalization layers \n",
        "# # to evaluation mode before running inference. Failing to do this will yield \n",
        "# # inconsistent inference results. If you wish to resuming training, \n",
        "# # call model.train() to ensure these layers are in training mode.\n",
        "\n",
        "# \"\"\" SAVING ON GPU/CPU \n",
        "# # 1) Save on GPU, Load on CPU\n",
        "# device = torch.device(\"cuda\")\n",
        "# model.to(device) # 모델을 GPU로 보냄\n",
        "# torch.save(model.state_dict(), PATH) # GPU로 보낸상태에서 파라미터 저장\n",
        "# device = torch.device('cpu')\n",
        "# model = Model(*args, **kwargs)\n",
        "# model.load_state_dict(torch.load(PATH, map_location=device)) # 파라미터를 CPU로 불러올때 저장된 위치(GPU) 전달\n",
        "\n",
        "# # 2) Save on GPU, Load on GPU\n",
        "# device = torch.device(\"cuda\")\n",
        "# model.to(device)\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "# model = Model(*args, **kwargs)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.to(device)\n",
        "# # Note: Be sure to use the .to(torch.device('cuda')) function \n",
        "# # on all model inputs, too!\n",
        "# # 3) Save on CPU, Load on GPU\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "# device = torch.device(\"cuda\")\n",
        "# model = Model(*args, **kwargs)\n",
        "# model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))  # Choose whatever GPU device number you want\n",
        "# model.to(device)\n",
        "# # This loads the model to a given GPU device. \n",
        "# # Next, be sure to call model.to(torch.device('cuda')) to convert the model’s parameter tensors to CUDA tensors\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "UkvYXQKrvDlU",
        "outputId": "bad4a728-f919-4954-d850-ec40e7ccc2dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "samples: 4, features: 1\n",
            "13.83521842956543\n",
            "9.605354309082031\n",
            "6.670308589935303\n",
            "4.633710861206055\n",
            "3.220527172088623\n",
            "2.2399163246154785\n",
            "1.5594604015350342\n",
            "1.0872749090194702\n",
            "0.7596040964126587\n",
            "0.5322098731994629\n",
            "0.3743947744369507\n",
            "0.264859676361084\n",
            "0.18882541358470917\n",
            "0.13603642582893372\n",
            "0.09937752783298492\n",
            "0.0739106610417366\n",
            "0.05621033161878586\n",
            "0.04389889910817146\n",
            "0.03532705083489418\n",
            "0.029350124299526215\n",
            "weights : 2.061, bias : -0.180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O_Sf9ae25tiV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}