{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiyong21c/pytorch_tutorial/blob/main/20220714_Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# autograd\n",
        "\n",
        "import torch\n",
        "# The autograd package provides automatic differentiation \n",
        "# for all operations on Tensors\n",
        "\n",
        "# requires_grad = True -> tracks all operations on the tensor. \n",
        "x = torch.randn(3, requires_grad=True) # [랜덤값1, 랜덤값2, 랜덤값3, requires_grad=True]\n",
        "y = x + 2\n",
        "\n",
        "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
        "# grad_fn: references a Function that has created the Tensor\n",
        "print(x) # tensor([-2.2015,  0.1272,  0.9258], requires_grad=True)\n",
        "print(x.grad_fn) # None : 직접만든 텐서로 연산이 수행된것이 아니므로\n",
        "\n",
        "# y는 x의 더하기연산의 결과로 생성됨\n",
        "print(y) # tensor([1.8301, 0.8254, 1.4925], grad_fn=<AddBackward0>) : y는 연산의 결과로 생성된 것이므로 grad_fn을 갖는다\n",
        "# grad_fn : 해당 텐서가 직전에 어떤연산으로 부터 생성 되었는지 알려준다\n",
        "print(y.grad_fn) # <AddBackward0 object at 0x7f491793cd10>\n",
        "\n",
        "# Do more operations on y\n",
        "z = y * y * 3\n",
        "print(z) # tensor([ 1.1012, 16.5231,  8.6291], grad_fn=<MulBackward0>) : 곱하기연산이 수행되었음\n",
        "z = z.mean() # z가 Scalar가 됨(Loss가 Scalar인경우)\n",
        "print(z) # tensor(8.7511, grad_fn=<MeanBackward0>) : 평균연산이 수행되었음\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Let's compute the gradients with backpropagation\n",
        "# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
        "# The gradient for this tensor will be accumulated into .grad attribute.\n",
        "# It is the partial derivate of the function w.r.t. the tensor\n",
        "\n",
        "z.backward() # z는 Scalar : z.backward(torch.tensor(1.))과 동일\n",
        "print(x.grad) # dz/dx : 기울기 연산결과 tensor([-0.3019,  3.2651,  5.6809])\n",
        "\n",
        "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
        "# It computes partial derivates(편미분) while applying the chain rule\n",
        "\n",
        "# -------------\n",
        "# Model with non-scalar output:\n",
        "# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \n",
        "# specify a gradient argument that is a tensor of matching shape.\n",
        "# needed for vector-Jacobian product\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2 # x의 연산결과로 나온 y\n",
        "for _ in range(10):\n",
        "    y = y * 2 # y의 연산결과로 나온 y\n",
        "\n",
        "print(y)\n",
        "print(y.shape) # y가 Scalar가 아닌 텐서 : torch.Size([3])\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32) # v(jacob vector) : Loss에 대한 Output의 기울기(dLoss/dy) 벡터로 backward시 파라미터로 전달하여, 가중치 또는 중요도를 조절할 수 있다.\n",
        "# [0.1, 1.0, 0.0001] 값이 중요한게 아니라 chain rule을 위해 곱해지는 것\n",
        "print(v.shape) # torch.Size([3])\n",
        "y.backward(v)\n",
        "print(x.grad) # dLoss/dx = (dLoss/dy) * (dy/dx) ## Loss는 y(output)를 통해 나온 값\n",
        "              # dy/dx = v * dy/dx\n",
        "\n",
        "# -------------\n",
        "# Stop a tensor from tracking history:\n",
        "# For example during our training loop when we want to update our weights\n",
        "# then this update operation should not be part of the gradient computation\n",
        "# - x.requires_grad_(False) : 텐서 만들때파라미터로 지정해도됨 requires_grad=True/False\n",
        "# - x.detach()\n",
        "# - wrap in 'with torch.no_grad():'\n",
        "\n",
        "# .requires_grad_(...) changes an existing flag in-place.\n",
        "a = torch.randn(2, 2) # requires_grad=True/False 파라미터 지정 안함\n",
        "print(a.requires_grad) # False\n",
        "b = ((a * 3) / (a - 1)) # a의 연산결과로 나온 b\n",
        "print(b.grad_fn) # None\n",
        "a.requires_grad_(True) # a = torch.randn(2, 2, requires_grad=True)와 같음\n",
        "print(a.requires_grad) # True\n",
        "b = a * a\n",
        "print(b.grad_fn) # <MulBackward0 object at 0x7f489f02aa10> : 직전연산에서 곱하기 연산 수행됨\n",
        "b = b.sum()\n",
        "print(b.grad_fn) # <SumBackward0 object at 0x7f48a670e990> : 직전연산에서 더하기 연산 수행됨\n",
        "\n",
        "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "print(a.requires_grad) # True\n",
        "b = a.detach() # .detach():기울기계산을 하지않는 텐서로 복사\n",
        "print(b.requires_grad) # False\n",
        "\n",
        "# wrap in 'with torch.no_grad():'\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "print(a.requires_grad) # True\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad) # False\n",
        "\n",
        "# -------------\n",
        "# backward() accumulates the gradient for this tensor into .grad attribute. : backward()수행시 grad가 += 연산됨(축적)\n",
        "# !!! We need to be careful during optimization !!!\n",
        "# Use .zero_() to empty the gradients before a new optimization step! : optimizer.step()시 기울기가 계산되므로 grad를 다시 초기화(zero_())\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "    # just a dummy example\n",
        "    model_output = (weights*3).sum()\n",
        "    model_output.backward()\n",
        "    \n",
        "    print(weights.grad)\n",
        "\n",
        "    # optimize model, i.e. adjust weights...\n",
        "    with torch.no_grad():\n",
        "        weights -= 0.1 * weights.grad # 옵티마이저가 하는 역할 : 가중치 수정\n",
        "\n",
        "    # this is important! It affects the final weights & output\n",
        "    weights.grad.zero_()\n",
        "\n",
        "print(weights)\n",
        "print(model_output)\n",
        "\n",
        "# # Optimizer has zero_grad() method\n",
        "# # optimizer = torch.optim.SGD([weights], lr=0.1) : [weights]자리에 model.parameters()을 넣는다.\n",
        "# # During training:\n",
        "# # optimizer.step() # 가중치(w) 갱신 : w = w - (lr * dLoss/dw)\n",
        "# # optimizer.zero_grad() # dLoss/dw → 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHeidAFmA79I",
        "outputId": "128b79ba-2d8e-44e7-b4f7-49c0766b10a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1176,  0.7011,  1.8788], requires_grad=True)\n",
            "None\n",
            "tensor([1.8824, 2.7011, 3.8788], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7fa2c501cc10>\n",
            "tensor([10.6302, 21.8885, 45.1350], grad_fn=<MulBackward0>)\n",
            "tensor(25.8846, grad_fn=<MeanBackward0>)\n",
            "tensor([3.7648, 5.4023, 7.7576])\n",
            "tensor([1209.3574, -110.4234,  879.3964], grad_fn=<MulBackward0>)\n",
            "torch.Size([3])\n",
            "torch.Size([3])\n",
            "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n",
            "False\n",
            "None\n",
            "True\n",
            "<MulBackward0 object at 0x7fa2c501ccd0>\n",
            "<SumBackward0 object at 0x7fa2c501ccd0>\n",
            "True\n",
            "False\n",
            "True\n",
            "False\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n",
            "tensor(4.8000, grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1FDbsExWI3fd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}