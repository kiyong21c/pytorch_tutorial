{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiyong21c/pytorch_tutorial/blob/main/20220706_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 0) Prepare data\n",
        "bc = datasets.load_breast_cancer() # sklearn를 통해 불러온 데이터셋\n",
        "X, y = bc.data, bc.target   # X.shape : (569, 30), y.shape : (569,)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test) # X_train으로 훈련하여 X_test에도 적용\n",
        "\n",
        "# numpy(array) → torch(tensor)\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# y_train/test 1차원 배열 → 2차원 배열(모델에 사용되기 위함)\n",
        "y_train = y_train.view(y_train.shape[0], 1) # view() : 넘파이의 reshape()과 같다\n",
        "y_test = y_test.view(y_test.shape[0], 1) # torch.size([114]) → torch.size([114, 1])\n",
        "\n",
        "# 1) Model\n",
        "# Linear model f = wx + b , sigmoid at the end\n",
        "class Model(nn.Module):\n",
        "    # nn.Module을 상속받으면서 사용자정의 모델 만들시 해야할 두가지\n",
        "\n",
        "    # 1)__init__()재정의(오버라이딩)\n",
        "    def __init__(self, n_input_features):\n",
        "        # super(Model, self).__init__()\n",
        "        super().__init__()   # # nn.Module 상속\n",
        "        self.linear = nn.Linear(n_input_features, 1) # 사용될 층 생성, 1)활성화 함수를 층으로 만들어도 되고, 2)forward에 적용해도 됨\n",
        "\n",
        "    # 2)forward 모듈 재정의(오버라이딩)\n",
        "    def forward(self, x):   # nn.Module 클래스에 있는 forward 모듈을 재정의(오버라이딩) 해야함\n",
        "    # 모델생성 후 self.forward() 하지 않아도 nn.Module의 영향으로 자동으로 호출되는 메서드\n",
        "        y_pred = torch.sigmoid(self.linear(x))  # torch가 제공하는 sigmoid()\n",
        "        return y_pred\n",
        "\n",
        "model = Model(n_features) # 보통은 input_features, output_features를 파라미터로 받지만, 이경우 output_features가 1이기 때문에 모델 class 정의할 때 부터 생략\n",
        "# 파라미터 확인\n",
        "w, b = model.parameters()\n",
        "print('최초 w =',w[0], '최초 b =',b[0])\n",
        "\n",
        "# 2) Loss and optimizer\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss() # Binary Classification Entrophy 손실함수 : 마지막레이어의 노드가 1개인 이진분류에 주로 사용됨(이경우 암진단결과)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # 옵티마이저에 모델의 파라미터(w, b), learning_rate 전달 \n",
        "\n",
        "# 3) Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_pred = model(X_train) # 모델에 훈련데이터 입력 → 예측값\n",
        "    loss = criterion(y_pred, y_train) # 손실함수(예측값, 실제값) → 손실값\n",
        "    # print(loss) # 1.0303 : 스칼라값\n",
        "\n",
        "    # Backward pass and update\n",
        "    loss.backward() # dloss/dw, dloss/db 계산됨\n",
        "    optimizer.step() # optimizer 내부 로직에 의해 모델의 파라미터(w, b) 갱신됨\n",
        "\n",
        "    # zero grad before new step\n",
        "    optimizer.zero_grad() # dloss/dw, dloss/db 초기화(0)\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# 파라미터 확인\n",
        "w, b = model.parameters()\n",
        "print('최종 w =',w[0], '최종 b =',b[0]) # 0번째 인덱스가 w,b 각각의 값\n",
        "print(w.shape, b.shape) # torch.Size([1, 30]) torch.Size([1])\n",
        "\n",
        "# 1.훈련 데이터셋으로 훈련된(w, b 갱신완료) 모델로 테스트 데이터의 타깃값 예측\n",
        "# 2.테스트 데이터의 타깃값(예측)과 실제 타깃값으로 정확도(accuracy) 확인\n",
        "with torch.no_grad():   # backward() 메서드 사용안하는데? no_grad()해야함?, w,b갱신안하는데..\n",
        "    y_predicted = model(X_test)\n",
        "    y_predicted_cls = y_predicted.round() # 반올림\n",
        "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "    # a.eq(b).sum() : (a == b).sum(), 같으면 더해라\n",
        "    print(f'accuracy: {acc.item():.4f}')"
      ],
      "metadata": {
        "id": "wjYRTvOmaQvV",
        "outputId": "954442aa-4280-45e4-a0d6-430b75f9d141",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최초 w = tensor([ 0.0029, -0.1791, -0.1629,  0.1317,  0.0757, -0.1802,  0.0287, -0.1818,\n",
            "        -0.0854, -0.1718, -0.0949, -0.1478, -0.0415, -0.0360,  0.0534,  0.0488,\n",
            "         0.0199, -0.0316,  0.1676, -0.0130, -0.0985,  0.0291, -0.0802,  0.0003,\n",
            "         0.1036, -0.0852,  0.0453, -0.0917, -0.0523,  0.1685],\n",
            "       grad_fn=<SelectBackward0>) 최초 b = tensor(0.0439, grad_fn=<SelectBackward0>)\n",
            "epoch: 10, loss = 0.4160\n",
            "epoch: 20, loss = 0.3691\n",
            "epoch: 30, loss = 0.3349\n",
            "epoch: 40, loss = 0.3089\n",
            "epoch: 50, loss = 0.2882\n",
            "epoch: 60, loss = 0.2713\n",
            "epoch: 70, loss = 0.2571\n",
            "epoch: 80, loss = 0.2451\n",
            "epoch: 90, loss = 0.2347\n",
            "epoch: 100, loss = 0.2256\n",
            "최종 w = tensor([-0.1152, -0.2415, -0.2822,  0.0173,  0.0095, -0.2622, -0.0745, -0.3040,\n",
            "        -0.1272, -0.1539, -0.1766, -0.1228, -0.1189, -0.1208,  0.0562,  0.0319,\n",
            "         0.0107, -0.0696,  0.1805,  0.0029, -0.2286, -0.0476, -0.2094, -0.1215,\n",
            "         0.0036, -0.1774, -0.0605, -0.2241, -0.1366,  0.1120],\n",
            "       grad_fn=<SelectBackward0>) 최종 b = tensor(0.1314, grad_fn=<SelectBackward0>)\n",
            "torch.Size([1, 30]) torch.Size([1])\n",
            "[Parameter containing:\n",
            "tensor([[-0.1152, -0.2415, -0.2822,  0.0173,  0.0095, -0.2622, -0.0745, -0.3040,\n",
            "         -0.1272, -0.1539, -0.1766, -0.1228, -0.1189, -0.1208,  0.0562,  0.0319,\n",
            "          0.0107, -0.0696,  0.1805,  0.0029, -0.2286, -0.0476, -0.2094, -0.1215,\n",
            "          0.0036, -0.1774, -0.0605, -0.2241, -0.1366,  0.1120]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([0.1314], requires_grad=True)]\n",
            "accuracy: 0.9035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e3Cqkjp2alsE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}