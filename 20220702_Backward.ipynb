{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiyong21c/pytorch_tutorial/blob/main/20220702_Backward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "# This is the parameter we want to optimize -> requires_grad=True\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# forward pass to compute loss_1\n",
        "y_predicted = w * x\n",
        "loss = (y_predicted - y)**2\n",
        "print('loss :', loss)\n",
        "\n",
        "# backward pass to compute gradient dLoss/dw\n",
        "loss.backward()\n",
        "print(w.grad)   # -2\n",
        "\n",
        "# learning_rate\n",
        "lr = 0.01\n",
        "\n",
        "# update parameter\n",
        "with torch.no_grad():\n",
        "    w -= lr * w.grad    # 1 → 1 - (-0.01)*(-2)\n",
        "\n",
        "print(w)   # 1.02\n",
        "print(f'before w.grad.zero_() :', w.grad) # -2\n",
        "w.grad.zero_()\n",
        "print(f'after w.grad.zero_() :', w.grad)  # 0\n",
        "print('w :',w) # 1.02 : w.grad가 0가 된것, w는 갱신된 상태\n",
        "\n",
        "# forward pass to compute loss_2\n",
        "y_predicted = w * x\n",
        "loss = (y_predicted - y)**2\n",
        "print('loss :', loss)\n",
        "\n",
        "# backward pass to compute gradient dLoss/dw\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# 1.순전파 : loss 계산\n",
        "\n",
        "# 2.역전파 : loss.backward() 하면서 w.grad(=dLoss/dw) 계산됨\n",
        "\n",
        "# 3.파라미터 업데이트(w.grad계산 꺼줌) : with torch.no_grad(): \n",
        "\n",
        "# 4.w.grad(dLoss/dw)를 0으로 초기화 : w.grad.zero_() , 갱신된 w를 통해서 새로운 y_predicted 및 loss가 계산되므로 역전파시 w.grad도 다시 계산되어야 하므로(0안하면 누적됨)"
      ],
      "metadata": {
        "id": "7ze5A6SogH5v",
        "outputId": "644201e0-494d-430d-d14a-bacc7171668f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss : tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n",
            "tensor(1.0200, requires_grad=True)\n",
            "before w.grad.zero_() : tensor(-2.)\n",
            "after w.grad.zero_() : tensor(0.)\n",
            "w : tensor(1.0200, requires_grad=True)\n",
            "loss : tensor(0.9604, grad_fn=<PowBackward0>)\n",
            "tensor(-1.9600)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}